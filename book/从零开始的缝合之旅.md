# 从零开始的缝合之旅

## Log

稍微整理了一下思路，解决了一些 K210 的陈年老坑（并没完全解决），现在决定将第二版、第三版、ucore、xv6 完全缝合起来。

但是理智告诉我在几天之内是肯定缝合不完的...

我还是继续在 multicore 分支上缝缝补补吧 QAQ。虽然那个代码我自己都有点看不下去了。但是仔细想了一下确实是从头再来更好...不然有点改不下去。

### 2020-10-18

如果不快点干应该干不完了...QAQAQQQ这几天要进入刷夜模式了。

规定一下不动的东西：

整个文件系统模块原封不动，但是可能需要加一些新的 syscall。

### 2020-10-19

凌晨。惊讶的发现从头开始又是走了一条老路，如果让我真正从头再来的话我肯定能写出来...但是现在没有时间，照着之前的代码复制也是陷入了陷阱。所以按照计划进度没有跟上，我们在之前的分支上实现 syscall！

## lab1：最小化内核&格式化输入

这回尝试默认变成 release 模式，有一个问题就是会多出一个 .eh_frame 段，要把它手动丢到 .data 里面去，否则就会直接取代入口点。

想了一下，暂时不实现 logger 了。最后再说。

## lab2：Trap

直接访问物理地址+只涉及内核态+实现 ebreak/时钟中断

trapframe 里面保存 sstatus/sepc 首先可以为用户/内核线程构造一个入口。还有就是能支持某种程度上的 trap 嵌套。最有可能出现的一种 trap 嵌套的情形是：用户通过 ecall trap 进来，然后由于这个 ecall 运行的时间可能很长，我们需要打开中断。这里保存 sstatus/sepc 只是能保证在 sret 之后能回到正确的地方。也可以说在 trap handler 可能覆盖掉 sstatus/sepc 故而需要保存。有点把自己绕进去了...

这样说起来，为何 scause 和 stval 无需保存？应该是因为在 ecall 的时候，我们通过 scause 完成分发，参数的话只需要读寄存器即可，我们之后都不会再用到 scause 和 stval；如果是其他 trap，我们根本不会尝试重新打开中断，也就不会产生 trap 嵌套，更无需保存了。

RISC-V 应该避免出现**中断**嵌套。这是目前我所了解到的。

这次我们直接放弃保存 x3 和 x4，为后续的多核做铺垫。（等等，那如果什么地方修改 x3 和 x4 要怎么办，忽然想到好像没有相关机制保证 x3 和 x4 不能使用。不过这个 thread pointer/global pointer 编译器应该不会乱用，姑且相信它吧）

另一个问题是，我们需要保存 sp 吗？当时这段汇编写的很复杂，希望能尝试简化一下...令人最不爽的点在于，sscratch 和 trapframe 里面的 sp 到底使用哪个。我们可以将奇怪的通过 trapframe 第一次进入用户态与其他独立起来起来，因为只有它非常奇怪（令人很不爽的是，它还可以通过 trapframe 进入内核态）。

剩下的可能比较简单，只需将 s 进入/退出 s，和 u 进入/退出 s 合并到一起。

现在没有时间仔细推敲 trap 的汇编代码了。只是照抄了第二版的代码之后把 breakpoint 和时钟中断跑起来了。略微调整了 k210 的频率常数，现在能做到接近一秒输出一次 100ticks。所以它的真正时钟频率只有 10M 吗...现在每个 tick 是 10000 个时钟周期，应该能看到比较明显的多核切换。

## lab3: 内存管理

这里实现物理页帧管理和内核堆内存分配即可。

# 续命的缝合之旅

## Log

### 2020-10-18

仔细想了一下，距离 22 号的 ddl 已经非常接近了QAQ，从零开始有点开玩笑了。那真就是只能改动现在的 multicore 分支，依次完成下面这些任务：

1. 将中断和调度之间的耦合删除，每个用户进程一个内核栈，对于中断、进程、调度三个方面都要进行改写QAQ。要保证修改之后还能够正常跑目前的三个应用程序。
2. 完善虚拟存储支持来跑比较大的用户程序。
3. 完整支持 fork/wait/exec 系统调用，能给函数传入命令行参数
4. 无需拔插 microsd 即可烧写文件系统的小工具
5. 完善文件系统支持，参考 xv6 提供 pipe 支持，从而终端可以通过 `|` 实现简单的 IPC。

那么就分成 4 个不同的 step 开始干活吧！

然后发现从中断开始就根本干不下去，应该不可能同时把中断机制换掉了。先按照原定的计划从头开始来，如果明天上午发现进度跟不上的话就切换到在原来的 multicore 分支上完善 syscall。

在进度的压力下，至少在 22 日之前我们不考虑 step 1 了！而且需要重新调整一下完成顺序：

1. 完整支持 fork/wait/exec 等 syscall，能够跑 xv6 的一些用户程序。
2. 完善文件系统，支持 pipe，从而能够跑 xv6 的更多用户程序。

### 2020-10-19

现在是晚上十点半。已经可以在内核里面通过硬编码跑一个 `forktest`，验证了 `fork` 和 `wait` 实现的正确性。

接下来就是要正确实现 `exec`，然后替换掉 user_shell 的实现。

### 2020-10-20

现在凌晨一点，user_shell 已经替换成基于 fork/exec 的实现。但是现在有一个问题就是 threadtrace 会出现某一个 hart 上面用时特别长的情况。看了一下是 exec syscall 的时间都计入了内核态，现在的实现是需要直接将所有页面拷贝到内存的，所以直接就几百万个时钟周期了...这个后续的实现我们还可以再改进一下。

### step1(已弃坑)

首先是根据我希望的命名，将 `interrupt` 目录改成 `trap`，同时将 `context.rs` 重命名为 `trapframe.rs`，将 `interrupt.asm` 重命名为 `trap.asm`。

接着，将 trap 分发入口 `handle_interrupt` 重命名为 `trap_handler`，将进入和退出 `trap` 的汇编代码重命名为 `__trapentry` 和 `__trapret`。

重要的是，`handler_interrupt` 不再需要返回一个 `*mut Context`。像是第二版一样，它仍然需要一个 `*mut Context` ，也就是位于内核栈顶的 TrapFrame 作为输入参数。在 `trap_handler` 里面，大概的逻辑是

### step1

首先是调整用户态的代码/数据段的位置。在 user 文件夹里面新增 linker.ld 将开头位置调整到 0。这样和 xv6 保持一致。此外根据先前的设计我们希望用户栈从 0x0c00_0000 向下增长，这样有 192M 的虚拟地址空间。目前来讲完全够用了。

发现之前的设计里面映射的 DEVICE_MMIO 区间实际上是给 alloc_page_range 使用的，跟串口不沾边。因为串口纯粹通过 sbi 接口，在 S 态根本不用映射。

现在我们调整了运行栈的位置，并替换了 alloc_page_range 函数为实际中会用到的 alloc_run_stack 函数。在 ProcessInner 维护一个 run_stack_pointer，每次从这个开始往下分配一个 run_stack 在加上一个 protect page。内核进程从最高的第二页往下，而用户进程从 0x0c00_0000 开始向下。

加上一个 Segmentation fault 机制：目前是只要在用户态触发 page fault 就直接杀死当前进程。目前加入了一个 stack_overflow 程序，在两个平台上可以正常跑了。

接下来，我们需要正确实现 fork/wait/exec/exit 的语义，尤其需要注意父子进程机制。

先定义一下父子进程机制：父进程通过 fork 可以创建子进程，在父进程里面维护子进程的一个 Arc。父进程可以通过 wait 得到一个已经退出的子进程的 PID 和返回值。在得到它之后，我们会将这个 Arc 删除掉，于是所有进程的所有引用计数全部消失，可以回收掉相应的资源（事实上，在进程退出的那一刻开始我们就应该回收掉）。

fork：将子进程的 Arc 放入父进程的子进程列表。

exit：将返回状态保存在当前进程的进程控制块中，回收掉用户态的所有物理页面（即用户栈和用户代码、数据段，即 0x0c00_0000 以下的部分），保留页表里面其他的映射。查看父进程的状态，如果它正在 waiting，则唤醒父进程。*这里需要特别小心锁的设计，包括：整个调度队列、ProcessInner，还有 wait 相关的 sleeplock 应该如何设计。*

wait: 

**Segment::map** 可能有 bug。可能是 xv6 应用太毒瘤了？怪事，xv6 内核里面判定了每个段的开头必须页对齐，但是反汇编出来就不是这么回事了...但是我们暂时不用管它。

fork 的实现方法：初始化 memory_set，然后把 0x0c00_0000 以下的所有段都复制一遍：由于我们在父进程的页表里面，这很好办，...现在地址空间就算是齐活了。然后需要构造一个合适的中断帧，就和父进程的中断帧一样就行了，但需要将返回值 a0 改成子进程 ID。这样好像就行了啊。现在加上了 fork 和 getpid，比想象中容易实现很多啊。下午先来写一个 log 区分用户和内核的输出。

Logging 模块终于加上了。现在要面对的就是父子进程的处理...

目前已经实现的机制：

异常或者调用 exit 之后在 PCB 里面更新 exited 和 xstate 状态，立即回收掉用于存储用户数据的所有物理页帧。

终端进程作为根进程，进程最终都会被移交给它来做回收。

在 fork 的时候更新父进程的 child 和自身的 parent。需要注意的是，作为 root 的 user_shell 里面的 parent 为 None，而在 fork 里面调用 Process::from_parent 的时候我们已经在子进程里面保存了父进程的 Weak。

正确实现 wait 调用：只需在 PCB 里面查找一个 exited=true 的子进程，取出 xstate，将对应的 `Arc<Process>` 丢掉，如果找了一遍都没有找到的话需要被阻塞。如果调用的时候发现子进程列表为空，则直接返回 -1。目前是返回 -2 的话需要回到循环开头 retry。之后将 Syscall::Park 的参数去掉之后可以稍微解决这个问题。

当一个进程退出时，如果发现父进程正处于等待子进程退出，也即 wait 状态，则唤醒父进程；这个我们可以通过在 ProcessInner 里面加入一个 condvar 来实现。

理论上来讲，Syscall::Park 无需带参数。反正它都是要回去重新执行。目前的 fork/exec 还有点问题不太支持。

尚未实现的机制：

在fork 的同时复制 Unix 资源。

当一个进程退出时，如果子进程列表不为空，则将子进程移交给一个专门的回收用户进程。但是目前，我们暂且假定每个进程要负责等待它 fork 出来的所有的子进程结束。



到这里为止，step1 要做的事情基本上做完了。

### step2

简单列一下 step2 的目标。

1. 支持 exec 的命令行参数；
2. 仔细看了一下 ucore，发现里面还用到了 gettimeofday, yield, 还有 sleep。
3. 提供文件系统的 syscall，尤其是需要支持 pipe。为此，在 fork 的时候需要正确复制 Inode。
4. 在此基础上，支持虚拟存储和物理页面的延迟分配。

### ucore 应用移植

我们需要将原来 ucore 的测试跑起来。那么首先就是需要将 ucore 的测试程序拿进来。

从 rcore-user 里面搬运了一下 rust 和 ucore 两个目录的构建逻辑，然后有一些奇怪的 bug...

将 find 替换成 lookup，因为我们不再只是在根目录下面了，而是需要形如 `rust/hello_world` 这样的输入，发现会在奇怪的地方出现死锁。

ucore 应用会出现 filesz < memsz 的情况，elf 库不会帮我们 padding 一些 0，只能靠我们自己来做。

在移植的时候似乎做了这些事情：

1. 处理 filesz < memsz 的情况，原先的第三版不能处理 filesz < memsz 但 filesz > 0 的情况
2. 在 trap 上下文中保存 gp，因为 ucore 用户程序用到了它
3. 将 ucore initcode.S 中对于 a0,a1 的初始化功能移除，稍后移动到内核
4. 更新 sys_wait 的语义，现有两个参数 pid 和 xstate，如果 pid = 0 的话则代表可以等待任意一个子进程结束，否则只能等待一个 pid 给定的子进程结束才可以返回；而 xstate 也有可能为 NULL，这个时候 kernel 不应该向里面写入子进程返回值，否则会 page fault。

目前可以成功运行的 ucore 程序：

* hello
* forktest
* divzero
* testbss
* faultread
* faultreadkernel
> 说明：以上两个测例没啥营养...而且 faultreadkernel 的地址目前并不在 kernel 之内。
> 

* exit：只需要实现 yield 即可（从这里开始还没有在 k210 上测试过）
* matrix：似乎只需要实现 yield 即可
* yield：需要实现 yield。
* badarg: 需要实现 yield，且需要判断传入的 `*xstate` 用户态可以访问；
* sleep: 需要实现 sleep 和 gettime_msec。
* forktree：需要实现 yield 和 sleep。sleep 的时间单位为一个 tick，也就是 10ms。
* spin: 需要实现 kill 和 yield。


暂时不能运行的测试程序以及相应的 feature：

* sleepkill：需要实现 sleep 和 kill。而且，在一个进程处于阻塞状态下被 kill 的时候，我们大概可以选择：将其标记为 kill，找到它所在的 condvar，并唤醒它。
* waitkill：需要实现 yield 和 kill，其中一个子进程会杀死父进程，所以需要更加完善的进程移交机制。我们需要初始化一个专门 `while (wait())` 的初始用户进程。然后如果一个进程在退出的时候发现它有一些子进程，直接将这些子进程移交给这个初始用户进程。

* priority：需要实现一个 ucore lab6 特有的设置进程优先级的操作...此外还需要 sleep。这个需要修改进程调度框架，目前应该先不考虑。
* sfs_filetest1：只读的方式打开一个硬编码的文件然后 stat，没啥用...
* ls：一大堆文件系统相关的 syscall
* sh：一个简单的 shell，支持重定向操作 `<,>` ，管道 `|`（现在的实现没用管道而是类似 dup）。进程调度相关的东西目前都应该已经搞定了。整体上这个要放到最后。


x86 独有，应该从 rv64 的测例中移除：
* badsegment
* softint

暂时在 k210 上还跑不起来的测例：

* testbss：数组开得太大了...
* forktest：fork 的数目太多了

对于测例进行的改动：

* forktree：目前 tutorial 要求每个进程负责等待所有他 fork 出来的子进程结束才能退出。因此，在 `forktree()` 结尾加入了一个 `while (wait() >= 0)` 来等待所有子进程结束并回收资源。此外，将 DEPTH 调整为 3 以适应 K210 的 8M 内存。
* 将 forktest 中的子进程数调整为 15，以适应 K210 的 8M 内存。
* 将 matrix 中的子进程数调整为 15，以适应 K210 的 8M 内存。
* testbss 之前开了 4M 的数组，太疯狂了，现在改成 1M。

> 注：目前 K210 上对于设备的映射会造成存储页表页面的极大浪费，现在一个进程会在内存中保留 60 个左右的页面，而整块内存的线性映射只会占用 7 个页帧。这个之后完成用户、内核地址空间的隔离会缓解这一问题。目前只能先将同时存在的子进程数调整的小一些。

### syscall 备注

* 现在的 sys_kill 可能还没办法 kill 掉一个正在阻塞的进程。现在的实现方法就是在进入 trap 的时候检查一下当前 process 的 killed 有没有被标记为 true。如果是的话则等同于触发了 page fault。

  事实上我们还是需要像传统的实现那样有一个 process table，我们需要根据 pid 查到进程的状态，如果是阻塞的话，需要保存指向 condvar 的指针。这样 kill 的话就可以把阻塞的状态取消，然后再回收掉进程。

  目前为了简单起见，我们搞一个 process table，但并不处理进程正在阻塞的情况。
  
* 准备开始创建 initproc 作为初始用户进程，它 fork 并 exec user_shell，随后一直 wait 来回收掉所有僵尸进程。但是如果想实现这个机制的话就需要将 parent 移动到 Process.Inner 中...假如父进程退出，需要修改子进程的 parent，而如果子进程这个时候正在 exit，需要尝试唤醒对应的父进程，这明显会产生并发冲突。但是这样做了之后发现内核进程报 page fault 了...还是在我删除了子进程转移相关逻辑的情况下。想正确实现并发真的很困难，直接将所有可变的东西丢到一个 Inner 并不够，事实上我们必须知道 Process 的某种“不变量”，在不变量可能不满足的中间状态必须全程持有 Inner 的锁。而之前的实现很有可能是修改了 Inner 中的某个地方就将锁释放掉，这个时候是破坏掉“不变量”的。一旦在多核的情况下，就很容易导致整个系统的崩溃。

  这里也许可以看出 Rust 相对于 C 的一个小缺点。在 C 里面我们显式获取并释放锁，而在 Rust 里面它是相对隐式的，这要求编程者在处理的时候要有更加清醒的头脑才行。

  上面的内核进程 page fault 是一个非常诡异的情况：

  ```
  Process 2 Segmentation Fault cause = Exception(InstructionPageFault), vaddr = 0x8, sepc = 0x8 @Core0
  ```

  甚至在一开始 __restore 到 idle 线程的时候就可能会报错。

  甚至，在 ProcessInner 里面加上一个初始化之后就不再使用的 parent_pid 都会爆炸，原先 usertests 可以跑，现在则可能出现死锁。这完全有点玄学...

